{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "# from nlpaug.util import Action\n",
    "from common_helpers import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset_path = \"irds:beir/fiqa\"\n",
    "dataset = pt.get_dataset(dataset_path)\n",
    "testset = pt.get_dataset(dataset_path + \"/test\")\n",
    "test_queries = testset.get_topics()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Indexing documents\n",
    "from pathlib import Path\n",
    "\n",
    "indexer = pt.IterDictIndexer(\n",
    "    str(Path.cwd()),  # this will be ignored\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    "    fields=[\"text\"]\n",
    ")\n",
    "index_ref = indexer.index(dataset.get_corpus_iter())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bm25 = pt.terrier.Retriever(index_ref, wmodel=\"BM25\")\n",
    "\n",
    "models = [bm25]\n",
    "metrics = [RR @ 10, nDCG @ 10, MAP @ 100]\n",
    "noise_levels = np.arange(0.0, 0.6, 0.1)\n",
    "noise_combinations = [\n",
    "    {\"sub\": 1.0, \"ins\": 0.0, \"del\": 0.0},  # Only substitution\n",
    "    {\"sub\": 0.0, \"ins\": 1.0, \"del\": 0.0},  # Only insertion\n",
    "    {\"sub\": 0.0, \"ins\": 0.0, \"del\": 1.0},  # Only deletion\n",
    "    {\"sub\": 0.5, \"ins\": 0.5, \"del\": 0.0},  # Substitution + Insertion\n",
    "    {\"sub\": 0.5, \"ins\": 0.0, \"del\": 0.5},  # Substitution + Deletion\n",
    "    {\"sub\": 0.0, \"ins\": 0.5, \"del\": 0.5},  # Insertion + Deletion\n",
    "    {\"sub\": 0.33, \"ins\": 0.33, \"del\": 0.33},  # Equal mix\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = []\n",
    "for noise_level in noise_levels:\n",
    "    for noise_config in noise_combinations:\n",
    "        # Set noise probabilities\n",
    "        sub_prob = noise_level * noise_config[\"sub\"]\n",
    "        ins_prob = noise_level * noise_config[\"ins\"]\n",
    "        del_prob = noise_level * noise_config[\"del\"]\n",
    "\n",
    "        noise_types = {\"substitute\": sub_prob, \"insert\": ins_prob, \"delete\": del_prob}\n",
    "        aug = naf.Sequential(\n",
    "            [nac.RandomCharAug(action=action, aug_char_p=0.25, aug_word_p=noise_type, \n",
    "                              spec_char=\"\", aug_char_min=0, aug_word_min=0, aug_char_max=500000, aug_word_max=500000)\n",
    "            for action, noise_type in noise_types.items()]\n",
    "        )\n",
    "\n",
    "        eval_result = run_noise_experiment(test_queries, testset, aug, models, metrics)\n",
    "\n",
    "        # Store results\n",
    "        eval_result[\"noise_level\"] = noise_level\n",
    "        eval_result[\"sub_prob\"] = sub_prob\n",
    "        eval_result[\"ins_prob\"] = ins_prob\n",
    "        eval_result[\"del_prob\"] = del_prob\n",
    "        results.append(eval_result)\n",
    "\n",
    "# Combine results\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "final_results.head(len(models))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_results[\"Noise Type\"] = final_results.apply(get_noise_label, axis=1)\n",
    "\n",
    "plot_metric(final_results, \"RR@10\", \"Reciprocal Rank at 10 (RR@10)\", \"Noise Type\")\n",
    "plot_metric(final_results, \"nDCG@10\", \"Normalized Discounted Cumulative Gain at 10 (nDCG@10)\", \"Noise Type\")\n",
    "plot_metric(final_results, \"AP@100\", \"Average Precision at 100 (AP@100)\", \"Noise Type\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate word count per query\n",
    "query_lengths = test_queries['query'].str.split().str.len()\n",
    "\n",
    "# Compute percentiles\n",
    "short_threshold = np.percentile(query_lengths, 25)  # 25th percentile\n",
    "long_threshold = np.percentile(query_lengths, 75)  # 75th percentile\n",
    "\n",
    "print(f\"Short query threshold: {short_threshold} words\")\n",
    "print(f\"Long query threshold: {long_threshold} words\")\n",
    "\n",
    "test_queries['query_length_category'] = test_queries['query'].apply(\n",
    "    lambda q: \"short\" if len(q.split()) <= short_threshold else\n",
    "              \"long\" if len(q.split()) > long_threshold else\n",
    "              \"medium\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "# Group queries by length\n",
    "query_groups = {\n",
    "    \"Short Queries\": test_queries[test_queries[\"query_length_category\"] == \"short\"],\n",
    "    \"Medium Queries\": test_queries[test_queries[\"query_length_category\"] == \"medium\"],\n",
    "    \"Long Queries\": test_queries[test_queries[\"query_length_category\"] == \"long\"]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    for noise_config in noise_combinations:\n",
    "        sub_prob = noise_level * noise_config[\"sub\"]\n",
    "        ins_prob = noise_level * noise_config[\"ins\"]\n",
    "        del_prob = noise_level * noise_config[\"del\"]\n",
    "\n",
    "        noise_types = {\"substitute\": sub_prob, \"insert\": ins_prob, \"delete\": del_prob}\n",
    "        aug = naf.Sequential([\n",
    "            nac.RandomCharAug(action=action, aug_char_p=0.25, aug_word_p=noise_type, spec_char=\"\")\n",
    "            for action, noise_type in noise_types.items()\n",
    "        ])\n",
    "\n",
    "        for group_name, queries in query_groups.items():\n",
    "            eval_result = run_noise_experiment(queries, testset, aug, models, metrics)\n",
    "\n",
    "            eval_result[\"noise_level\"] = noise_level\n",
    "            eval_result[\"sub_prob\"] = sub_prob\n",
    "            eval_result[\"ins_prob\"] = ins_prob\n",
    "            eval_result[\"del_prob\"] = del_prob\n",
    "            eval_result[\"query_group\"] = group_name\n",
    "            results.append(eval_result)\n",
    "\n",
    "# Combine results\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "final_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ensure the final results are in a DataFrame\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Convert the result columns to proper types for plotting\n",
    "final_results[\"noise_level\"] = final_results[\"noise_level\"].astype(float)\n",
    "final_results[\"sub_prob\"] = final_results[\"sub_prob\"].astype(float)\n",
    "final_results[\"ins_prob\"] = final_results[\"ins_prob\"].astype(float)\n",
    "final_results[\"del_prob\"] = final_results[\"del_prob\"].astype(float)\n",
    "\n",
    "# Create a new column to represent the noise type as a string\n",
    "def get_noise_type(row):\n",
    "    if row['sub_prob'] > 0:\n",
    "        return 'Substitution'\n",
    "    elif row['ins_prob'] > 0:\n",
    "        return 'Insertion'\n",
    "    elif row['del_prob'] > 0:\n",
    "        return 'Deletion'\n",
    "    else:\n",
    "        return 'No Noise'\n",
    "\n",
    "final_results['noise_type'] = final_results.apply(get_noise_type, axis=1)\n",
    "\n",
    "# Plotting function for metrics with facet grid\n",
    "def plot_metric(metric):\n",
    "    # Create a FacetGrid for each noise type\n",
    "    g = sns.FacetGrid(final_results, col=\"noise_type\", hue=\"query_group\", height=5, aspect=1.5)\n",
    "\n",
    "    # Plot the results for each facet\n",
    "    g.map(sns.lineplot, \"noise_level\", metric, marker=\"o\")\n",
    "\n",
    "    # Add titles, axis labels, and grid\n",
    "    g.set_axis_labels('Noise Level', metric)\n",
    "    g.set_titles(col_template=\"{col_name} Noise\")\n",
    "    g.add_legend(title='Query Group')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example plots for different metrics\n",
    "plot_metric(\"RR@10\")\n",
    "plot_metric(\"nDCG@10\")\n",
    "plot_metric(\"AP@100\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tfidf = pt.terrier.Retriever(index_ref, wmodel=\"TF_IDF\")\n",
    "\n",
    "# Run Experiment\n",
    "retrievers = {\n",
    "    \"TF-IDF\": tfidf,\n",
    "    \"BM25\": bm25,\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    for noise_config in noise_combinations:\n",
    "        sub_prob = noise_level * noise_config[\"sub\"]\n",
    "        ins_prob = noise_level * noise_config[\"ins\"]\n",
    "        del_prob = noise_level * noise_config[\"del\"]\n",
    "\n",
    "        aug = naf.Sequential([\n",
    "            nac.RandomCharAug(action=\"substitute\", aug_char_p=0.25, aug_word_p=sub_prob, spec_char=\"\"),\n",
    "            nac.RandomCharAug(action=\"insert\", aug_char_p=0.25, aug_word_p=ins_prob, spec_char=\"\"),\n",
    "            nac.RandomCharAug(action=\"delete\", aug_char_p=0.25, aug_word_p=del_prob, spec_char=\"\")\n",
    "        ])\n",
    "\n",
    "        noisy_queries_list = []\n",
    "        for _, row in test_queries.iterrows():\n",
    "            noisy_query = aug.augment(row['query'])\n",
    "            noisy_query = \" \".join(noisy_query) if isinstance(noisy_query, list) else noisy_query\n",
    "            noisy_queries_list.append({'qid': row['qid'], 'query': noisy_query})\n",
    "\n",
    "        noisy_queries_df = pd.DataFrame(noisy_queries_list)\n",
    "        noisy_queries_df[\"qid\"] = noisy_queries_df[\"qid\"].astype(str)\n",
    "\n",
    "        for model_name, retriever in retrievers.items():\n",
    "            eval_result = pt.Experiment(\n",
    "                [retriever],\n",
    "                noisy_queries_df,\n",
    "                testset.get_qrels(),\n",
    "                eval_metrics=[RR @ 10, nDCG @ 10, MAP @ 100]\n",
    "            )\n",
    "\n",
    "            eval_result[\"model\"] = model_name\n",
    "            eval_result[\"noise_level\"] = noise_level\n",
    "            eval_result[\"sub_prob\"] = sub_prob\n",
    "            eval_result[\"ins_prob\"] = ins_prob\n",
    "            eval_result[\"del_prob\"] = del_prob\n",
    "            results.append(eval_result)\n",
    "\n",
    "# Combine results\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "print(final_results)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter results for BM25 and TF-IDF\n",
    "comparison_results = final_results[final_results[\"model\"].isin([\"BM25\", \"TF-IDF\"])]\n",
    "\n",
    "# Plotting function for metrics\n",
    "def plot_comparison(metric, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(\n",
    "        data=comparison_results,\n",
    "        x=\"noise_level\",\n",
    "        y=metric,\n",
    "        hue=\"model\",\n",
    "        marker=\"o\"\n",
    "    )\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Noise Level\", fontsize=12)\n",
    "    plt.ylabel(metric, fontsize=12)\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics\n",
    "plot_comparison(\"RR@10\", \"Reciprocal Rank at 10 (RR@10) Comparison\")\n",
    "plot_comparison(\"nDCG@10\", \"Normalized Discounted Cumulative Gain at 10 (nDCG@10) Comparison\")\n",
    "plot_comparison(\"AP@100\", \"Average Precision at 100 (AP@100) Comparison\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter results for TF-IDF and BM25\n",
    "comparison_results = final_results[final_results[\"model\"].isin([\"BM25\", \"TF-IDF\"])]\n",
    "\n",
    "comparison_results[\"Noise Type\"] = comparison_results.apply(get_noise_type, axis=1)\n",
    "\n",
    "# Plotting function for metrics\n",
    "def plot_noise_impact(metric, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(\n",
    "        data=comparison_results,\n",
    "        x=\"noise_level\",\n",
    "        y=metric,\n",
    "        hue=\"Noise Type\",\n",
    "        style=\"model\",\n",
    "        markers=True,\n",
    "        dashes=False\n",
    "    )\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Noise Level\", fontsize=14)\n",
    "    plt.ylabel(metric, fontsize=14)\n",
    "    plt.legend(title=\"Noise Type\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics\n",
    "plot_noise_impact(\"RR@10\", \"Impact of Noise on RR@10 for TF-IDF and BM25\")\n",
    "plot_noise_impact(\"nDCG@10\", \"Impact of Noise on nDCG@10 for TF-IDF and BM25\")\n",
    "plot_noise_impact(\"AP@100\", \"Impact of Noise on AP@100 for TF-IDF and BM25\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
